{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TKO_3120 Machine Learning and Pattern Recognition\n",
    "\n",
    "Image recognition exercise\n",
    "\n",
    "Sam Student <br>\n",
    "sam.student@utu.fi\n",
    "\n",
    "January 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This is the template for the image recognition exercise. <Br>\n",
    "Some **general instructions**:\n",
    " - write a clear *report*, understandable for an unspecialized reader: define shortly the concepts and explain the phases you use\n",
    "    - use the Markdown feature of the notebook for larger explanations\n",
    " - return your output as a working Jupyter notebook\n",
    " - name your file as MLPR20_exercise_surname.jpynb\n",
    " - write easily readable code with comments     \n",
    "     - if you exploit some code from web, provide a reference\n",
    "     - avoid redundant code! Exploit the relevent parts and modify the code for your purposes to produce only what you need \n",
    " - it is ok to discuss with a friend about the assignment. But it is not ok to copy someone's work. Everyone should submit their own implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deadline 13th of March at 16:00**\n",
    "- No extension granted, unless you have an extremely justified reason. In such case, ask for extension well in advance!\n",
    "- Start now, do not leave it to the last minute. This exercise will need some labour!\n",
    "- If you encounter problems, Google first and if you canâ€™t find an answer, ask for help\n",
    "    - pekavir@utu.fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grading**\n",
    "\n",
    "The exercise covers a part of the grading in this course. The course exam has 5 questions, 6 points of each. Exercise gives 4 points, i.e. the total score is 34 points. Two extra points can be acquired by completing the bonus task. <br>\n",
    "\n",
    "From the template below, you can see how many exercise points can be acquired from each task. Exam points are given according to the table below: <br>\n",
    "<br>\n",
    "7-8 exercise points: 1 point <br>\n",
    "9-10 exercise points: 2 points <br>\n",
    "11-12 exercise points: 3 points <br>\n",
    "13-14 exercise points: 4 points <br>\n",
    "<br>\n",
    "To pass the exercise, you need at least 7 exercise points, distributed somewhat evenly into tasks (you can't just implement Introduction, Data preparation and Feature extraction and leave the left undone!) <Br>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write an introductory chapter for your report **(1 p)**\n",
    "<br>E.g.\n",
    "- What is the purpose of this task?\n",
    "- What kind of data were used? Where did it originate?\n",
    "- Which methods did you use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images: https://unsplash.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import io, transform, color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(img):\n",
    "    plt.subplots(figsize=(8, 6))\n",
    "    plt.imshow(img)\n",
    "\n",
    "def resize_img(img, shape):\n",
    "    return transform.resize(img, shape, preserve_range=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_channel_mean(img, channel_index):\n",
    "    channel = img[:, :, channel_index]\n",
    "    return np.mean(channel)\n",
    "\n",
    "def img_r_mean(img):\n",
    "    return img_channel_mean(img, 0)\n",
    "\n",
    "def img_g_mean(img):\n",
    "    return img_channel_mean(img, 1)\n",
    "\n",
    "def img_b_mean(img):\n",
    "    return img_channel_mean(img, 2)\n",
    "\n",
    "def img_channel_variance(img, channel_index):\n",
    "    channel = img[:, :, channel_index]\n",
    "    return np.var(channel)\n",
    "\n",
    "def img_r_variance(img):\n",
    "    return img_channel_variance(img, 0)\n",
    "\n",
    "def img_g_variance(img):\n",
    "    return img_channel_variance(img, 1)\n",
    "\n",
    "def img_b_variance(img):\n",
    "    return img_channel_variance(img, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import texture\n",
    "\n",
    "def glcm_correlation(img, pixel_distances, angles=[0, np.pi / 2], levels=8):\n",
    "    grayscaled = color.rgb2gray(img).astype(int)\n",
    "    reduced = np.floor((grayscaled / 255) * levels).astype(int)\n",
    "    glcms = texture.greycomatrix(reduced, pixel_distances, angles=angles, levels=levels, normed=True, symmetric=True)\n",
    "    correlations = texture.greycoprops(glcms, 'correlation')\n",
    "    return np.array([correlations[:, 0], correlations[:, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(img):\n",
    "    return resize_img(img, (300, 300))\n",
    "\n",
    "def extract_features(img):\n",
    "    correlations = glcm_correlation(img, [2, 3])\n",
    "    horizontal_smoothness = correlations[:, 0]\n",
    "    vertical_smoothness = correlations[:, 1]\n",
    "    return (\n",
    "        img_r_mean(img),\n",
    "        img_g_mean(img),\n",
    "        img_b_mean(img),\n",
    "        img_r_variance(img),\n",
    "        img_g_variance(img),\n",
    "        img_b_variance(img),\n",
    "        *horizontal_smoothness,\n",
    "        *vertical_smoothness\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    (0, 'sand.txt'), \n",
    "    (1, 'grass.txt'), \n",
    "    (2, 'stairs.txt')\n",
    "]\n",
    "\n",
    "input_array = []\n",
    "for cat_id, cat_urls_file in categories:\n",
    "    cat_urls = np.loadtxt(cat_urls_file, dtype='U150')\n",
    "    \n",
    "    for url in cat_urls:\n",
    "        img = io.imread(url)\n",
    "        preprocessed = preprocess_img(img)\n",
    "        features = extract_features(preprocessed)\n",
    "        \n",
    "        input_array.append(\n",
    "            [*features, cat_id]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_index = [\n",
    "    'R mean',  'G mean', 'B mean', \n",
    "    'R variance', 'G variance', 'B variance', \n",
    "    'Horizontal smoothness #1', 'Horizontal smoothness #2',\n",
    "    'Vertical smoothness #1', 'Vertical smoothness #2',\n",
    "    'Category'\n",
    "]\n",
    "\n",
    "data = pd.DataFrame(input_array, columns=feature_index)\n",
    "\n",
    "X_train = data.drop('Category', axis=1, inplace=False)\n",
    "Y_train = data['Category'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R mean</th>\n",
       "      <th>G mean</th>\n",
       "      <th>B mean</th>\n",
       "      <th>R variance</th>\n",
       "      <th>G variance</th>\n",
       "      <th>B variance</th>\n",
       "      <th>Horizontal smoothness #1</th>\n",
       "      <th>Horizontal smoothness #2</th>\n",
       "      <th>Vertical smoothness #1</th>\n",
       "      <th>Vertical smoothness #2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>194.776410</td>\n",
       "      <td>165.387687</td>\n",
       "      <td>142.901508</td>\n",
       "      <td>563.559045</td>\n",
       "      <td>597.253541</td>\n",
       "      <td>507.250026</td>\n",
       "      <td>0.602046</td>\n",
       "      <td>0.607513</td>\n",
       "      <td>0.582189</td>\n",
       "      <td>0.592732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>203.179267</td>\n",
       "      <td>177.127218</td>\n",
       "      <td>141.848734</td>\n",
       "      <td>162.902561</td>\n",
       "      <td>150.384857</td>\n",
       "      <td>125.779320</td>\n",
       "      <td>0.683536</td>\n",
       "      <td>0.168246</td>\n",
       "      <td>0.600512</td>\n",
       "      <td>-0.013596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>209.286086</td>\n",
       "      <td>153.585664</td>\n",
       "      <td>127.026846</td>\n",
       "      <td>1664.624177</td>\n",
       "      <td>2394.619500</td>\n",
       "      <td>2224.736896</td>\n",
       "      <td>0.973603</td>\n",
       "      <td>0.907315</td>\n",
       "      <td>0.965525</td>\n",
       "      <td>0.853929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>178.151341</td>\n",
       "      <td>127.762167</td>\n",
       "      <td>82.454480</td>\n",
       "      <td>663.540706</td>\n",
       "      <td>486.874587</td>\n",
       "      <td>362.943892</td>\n",
       "      <td>0.527914</td>\n",
       "      <td>0.515529</td>\n",
       "      <td>0.482018</td>\n",
       "      <td>0.473186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>204.377670</td>\n",
       "      <td>185.428245</td>\n",
       "      <td>176.172585</td>\n",
       "      <td>3466.546247</td>\n",
       "      <td>3284.625045</td>\n",
       "      <td>4088.892210</td>\n",
       "      <td>0.961862</td>\n",
       "      <td>0.945046</td>\n",
       "      <td>0.951476</td>\n",
       "      <td>0.927518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       R mean      G mean      B mean   R variance   G variance   B variance  \\\n",
       "0  194.776410  165.387687  142.901508   563.559045   597.253541   507.250026   \n",
       "1  203.179267  177.127218  141.848734   162.902561   150.384857   125.779320   \n",
       "2  209.286086  153.585664  127.026846  1664.624177  2394.619500  2224.736896   \n",
       "3  178.151341  127.762167   82.454480   663.540706   486.874587   362.943892   \n",
       "4  204.377670  185.428245  176.172585  3466.546247  3284.625045  4088.892210   \n",
       "\n",
       "   Horizontal smoothness #1  Horizontal smoothness #2  Vertical smoothness #1  \\\n",
       "0                  0.602046                  0.607513                0.582189   \n",
       "1                  0.683536                  0.168246                0.600512   \n",
       "2                  0.973603                  0.907315                0.965525   \n",
       "3                  0.527914                  0.515529                0.482018   \n",
       "4                  0.961862                  0.945046                0.951476   \n",
       "\n",
       "   Vertical smoothness #2  \n",
       "0                0.592732  \n",
       "1               -0.013596  \n",
       "2                0.853929  \n",
       "3                0.473186  \n",
       "4                0.927518  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R mean</th>\n",
       "      <th>G mean</th>\n",
       "      <th>B mean</th>\n",
       "      <th>R variance</th>\n",
       "      <th>G variance</th>\n",
       "      <th>B variance</th>\n",
       "      <th>Horizontal smoothness #1</th>\n",
       "      <th>Horizontal smoothness #2</th>\n",
       "      <th>Vertical smoothness #1</th>\n",
       "      <th>Vertical smoothness #2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.455948</td>\n",
       "      <td>1.010815</td>\n",
       "      <td>0.885207</td>\n",
       "      <td>-1.046247</td>\n",
       "      <td>-1.015690</td>\n",
       "      <td>-0.798879</td>\n",
       "      <td>-1.393938</td>\n",
       "      <td>-1.153666</td>\n",
       "      <td>-1.136227</td>\n",
       "      <td>-0.834876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.628542</td>\n",
       "      <td>1.301463</td>\n",
       "      <td>0.863814</td>\n",
       "      <td>-1.280732</td>\n",
       "      <td>-1.309825</td>\n",
       "      <td>-0.954126</td>\n",
       "      <td>-0.900349</td>\n",
       "      <td>-3.678757</td>\n",
       "      <td>-1.037291</td>\n",
       "      <td>-3.917267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.753977</td>\n",
       "      <td>0.718619</td>\n",
       "      <td>0.562632</td>\n",
       "      <td>-0.401846</td>\n",
       "      <td>0.167360</td>\n",
       "      <td>-0.099911</td>\n",
       "      <td>0.856591</td>\n",
       "      <td>0.569717</td>\n",
       "      <td>0.933590</td>\n",
       "      <td>0.492969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.114468</td>\n",
       "      <td>0.079278</td>\n",
       "      <td>-0.343082</td>\n",
       "      <td>-0.987732</td>\n",
       "      <td>-1.088343</td>\n",
       "      <td>-0.857607</td>\n",
       "      <td>-1.842957</td>\n",
       "      <td>-1.682430</td>\n",
       "      <td>-1.677097</td>\n",
       "      <td>-1.442613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.653158</td>\n",
       "      <td>1.506981</td>\n",
       "      <td>1.561278</td>\n",
       "      <td>0.652732</td>\n",
       "      <td>0.753173</td>\n",
       "      <td>0.658747</td>\n",
       "      <td>0.785481</td>\n",
       "      <td>0.786612</td>\n",
       "      <td>0.857733</td>\n",
       "      <td>0.867073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     R mean    G mean    B mean  R variance  G variance  B variance  \\\n",
       "0  1.455948  1.010815  0.885207   -1.046247   -1.015690   -0.798879   \n",
       "1  1.628542  1.301463  0.863814   -1.280732   -1.309825   -0.954126   \n",
       "2  1.753977  0.718619  0.562632   -0.401846    0.167360   -0.099911   \n",
       "3  1.114468  0.079278 -0.343082   -0.987732   -1.088343   -0.857607   \n",
       "4  1.653158  1.506981  1.561278    0.652732    0.753173    0.658747   \n",
       "\n",
       "   Horizontal smoothness #1  Horizontal smoothness #2  Vertical smoothness #1  \\\n",
       "0                 -1.393938                 -1.153666               -1.136227   \n",
       "1                 -0.900349                 -3.678757               -1.037291   \n",
       "2                  0.856591                  0.569717                0.933590   \n",
       "3                 -1.842957                 -1.682430               -1.677097   \n",
       "4                  0.785481                  0.786612                0.857733   \n",
       "\n",
       "   Vertical smoothness #2  \n",
       "0               -0.834876  \n",
       "1               -3.917267  \n",
       "2                0.492969  \n",
       "3               -1.442613  \n",
       "4                0.867073  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "standartized = scaler.fit_transform(X_train)\n",
    "\n",
    "X_train = pd.DataFrame(standartized, columns=X_train.columns)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform preparations for the data **(3 p)**\n",
    "- import all the packages needed for this notebook in one cell\n",
    "- read the URL:s from the text files and import the images\n",
    "- crop and/or resize the images into same size\n",
    "- for GLCM and GLRLM, change the images into grayscale and reduce the quantization level, e.g. to 8 levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First order texture measures (6 features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate the below mentioned color features for each image **(1 p)**\n",
    "    - Mean for each RGB color channel\n",
    "    - Variance for each RGB color channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second order texture measures (4 or 10 features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gray-Level-Co-Occurrence (GLCM) features (4 features) **(2 p)**\n",
    "    - For each image\n",
    "        - calculate the GLC matrix\n",
    "        - calculate the \"correlation\" feature using the GLC matrix that you acquired \n",
    "            - in horizontal and vertical directions for two reference pixel distances (you can choose the distances)\n",
    "        - explain your choise for the distances<br>\n",
    "<br>\n",
    "- Gray-Level-Run-Length (GLRL) features (6 features) **(BONUS)**\n",
    "    - Make an own implementation for Gray-Level-Run-Length (GLRL) matrix in\n",
    "        - horizontal direction\n",
    "        - vertical direction\n",
    "        - test that your code works with the toy image: [[1,1,1,0],[2,0,0,1],[1,0,2,2],[0,0,0,0]]\n",
    "    - Implement the following run-length features using the GLRL matrix that you acquired\n",
    "        - Short-Run emphasis\n",
    "        - Long-run emphasis\n",
    "        - Run percentage\n",
    "    - Include the resulting features in the input array X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather your features into an input array X, and the image classes into an output array y. Standardize the feature values in X."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make illustrations of the feature relationships, and discuss the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairplot\n",
    "\n",
    "- Pairplot **(1 p)**\n",
    "    - Which feature pairs possess roughly linear dependence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PCA **(1 p)**\n",
    "    - Can you see any clusters in PCA?\n",
    "    - Does this figure give you any clues, how well you will be able to classify the image types? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build classifiers and estimate their performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- k Nearest Neighbors classifier **(1 p)**\n",
    "    - optimize the hyperparameter (k) and select the best model for the classifier\n",
    "    - estimate the performance of the model with nested cross validation\n",
    "    - calculate the accuracy and the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regularized linear model with Ridge regression **(1 p)**\n",
    "    - optimize the hyperparameter (alpha) and select the best model for the classifier\n",
    "    - estimate the performance of the model with nested cross validation\n",
    "    - calculate the accuracy and the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Multi-layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Multi-layer perceptron MLP **(2 p)**\n",
    "    - build the classifier. Use:\n",
    "        - 1 hidden layer\n",
    "        - solver for weight optimization: stochastic gradient-based optimizer ('adam')\n",
    "        - activation function for the hidden layer: rectified linear unit function ('relu')\n",
    "        - Early stop\n",
    "    - optimize the number of neurons in the hidden layer and select the best model for the classifier\n",
    "    - use Early stop committee, i.e. after selecting the model, calculate the prediction for the test data several times with different sampling of the training data. The members of the committee vote for the predicted class of the test sample. Use 50% of the training data for validation (algorithm terminates the training when validation score is not improving)\n",
    "    - estimate the performance of the classifier with nested cross validation\n",
    "    - calculate the accuracy and the confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Discuss you results **(1 p)**\n",
    "<br>E.g.\n",
    "    - Which model performs the best and why?\n",
    "    - What are the limitations?\n",
    "    - How could the results be improved?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
